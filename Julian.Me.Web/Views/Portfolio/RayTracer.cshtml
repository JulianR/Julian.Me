@{
    ViewBag.Title = "RayTracer";
}
<div id="raytracer-sections">
    <section title="overview">
        <p>
            This is a <a href="http://en.wikipedia.org/wiki/Ray_tracing_%28graphics%29">raytracing</a>
            engine without a special name that has been written fully in C# using XNA. While
            XNA is of course good for 3D rendering, no actual rendering is being done by XNA,
            it's just used for the convenience of handling user input and drawing the fully
            rendered frame to the screen as well as compiling and calling post-processing shaders.</p>
        <p>
            It's real-time enough that you're able to fly through a fairly simple scene with
            smooth framerates. As with all things raytracing, it benefits greatly from having
            more CPU cores.
        </p>
        <p>
            The greatest challenge of this project was of course to make it run as fast as possible.
            The natural question is of course why I didn't go with C or C++ for this. Well,
            mostly because I know and like C# and was curious how well it could handle this.
            I found out that it performed pretty well. In my experience, the biggest bottleneck
            that keeps a C# raytracer from running as fast as a C++ one is that with C# you
            cannot vectorize your code leveraging SIMD instructions (SSE and such). Which is
            a biggie because a C++ raytracer that does is able to process up to four rays at
            the same time. Mono does offer Mono.SIMD and I hope that Microsoft will add some
            way to work with SIMD as well in the future.
        </p>
        <p>
            It's still fairly naieve though. The algorithm currently is a broad-sweep phase
            of iterating through all scene objects per ray and doing a bounding-sphere intersection
            check before checking the triangles of the scene objects individually. This is fast
            for a small scene but doesn't scale to scenes with many objects or objects with
            many triangles.
        </p>
        <h3>
            Features implemented</h3>
        <ul>
            <li><strong>Triangles.</strong> Planes and triangles are the only primitives that are
                supported. Rudimentary support for importing .OBJ model files.</li>
            <li><strong>Soft-shadows.</strong> Normal lights cast harsh shadows but there is support
                for area lights that cast a soft shadow. This works by giving those lights a volume
                and doing multiple samples (up to 128 samples) per pixel from random points of origin
                within that volume. Naturally, there's a huge performance penalty to this. I have
                plans to move shadow rendering to the GPU, making them pretty much free.</li>
            <li><strong>Reflections.</strong> Raytracers make doing reflections really easy so naturally
                there's support for this! </li>
            <li><strong>Interactive.</strong> As it's a real-time raytracer, you can fly around
                freely in the scene and add lights and turn reflections on or off.</li>
            <li><strong>Postprocessing effects.</strong> There is support for postprocessing effects
                that can add things like <a href="http://en.wikipedia.org/wiki/Ambient_occlusion">ambient
                    occlusion</a>, <a href="http://en.wikipedia.org/wiki/Depth_of_field">depth of field</a>
                and <a href="http://en.wikipedia.org/wiki/Motion_blur">motion blur</a>. Though because
                work has been moved to the future version they are
                not in a functional state right now. </li>
        </ul>
    </section>
    <section title="future work">
        <p>
            This project was a great learning experience for me as it was my first encounter
            with vector math. And as I love the simple and elegant concept of raytracing, I
            definitely want to continue working on a project like this in the future.</p>
        <p>
            However, I'm running into some limitations with how it's set up. For one, it's not
            very scalable as the ray-intersection algorithm is fairly naive and can't deal with
            complex scenes or objects. For this, I need to implement an intersection algorithm
            that uses a more scalable basis, such as an octree.
        </p>
        <h3>
            SlimDX</h3>
        <p>
            And while working with XNA is a joy, it is a bit of a heavy dependency to take on
            for something that just handles user input and that presents a rendered bitmap to
            screen. I would like to start over with something that is based on <a href="http://slimdx.org/">
                SlimDX</a>. I still need a framework like that to provide me access to the GPU,
            to be able to work with shaders.
        </p>
        <h3>
            GPU</h3>
        <p>
            I want to explore the potential of harnessing the GPU to a greater extent in this
            rewrite. While the best performance might be had by moving the entire thing to the
            GPU (through something like CUDA) that's not really where my interest lies and I
            want to keep the core ray-intersection algorithm to the CPU, but have the GPU act
            as the composer of what the CPU produces.
        </p>
        <h3>
            Deferred rendering</h3>
        <p>
            It would in that sense be like a game engine (though those use rasterization rather
            than raytracing) that uses <a href="http://en.wikipedia.org/wiki/Deferred_Rendering">
                deferred rendering</a>: the engine renders everything to a collection of buffers
            that are then composited to form the final frame that is shown on screen.
        </p>
        <p>
            Except that in the case of my raytracer, the creation of those buffers (normals
            buffer, depth buffer, shadow buffer, etc) is done by the CPU rather than a game
            where mostly everything is done by the GPU.</p>
        <h3>
            Deferred soft shadowing</h3>
        <p>
            One example of what I want to try out is to render soft shadows in a shader on the
            GPU rather than perform very expensive multi-ray sampling on the CPU.
        </p>
        <p>
            The idea is that for every light within the scene, a texture of the scene is generated
            (on the CPU) that provides information on how far away a given pixel is from the
            light. An example of what that buffer looks like can be seen below. In it, the more
            blue a pixel is, the closer it is to a light (or when fully blue it's hidden from
            the light entirely). When we need to determine how "well-lit" a pixel needs to be,
            we first sample the light-distance texture for that pixel. The further away a pixel
            is from a light, the more diffuse the light would become. This distance that we
            sample then determines how many samples and how far away from the given pixel we
            then sample from the shadow buffer (see the second image). The average color of
            all those samples will then be the color of the pixel. Overall, this should result
            in a fairly realistic, soft shadow.
        </p>
        <p>
            I do expect some difficulties with this approach, mainly that the wrong pixels will
            be sampled in the shadow buffer, because what is near a pixel from the camera's
            view is not necessarily anywhere near that pixel in the scene. But I expect that
            it can be mostly worked around by involving the depth buffer or the normal buffer
            as well.
        </p>
        <p>
            Rendering artifacts aside, the benefit of this approach should be greatly improved
            performance of soft shadows. The only cost here is extra memory writes as a result
            from the need to fill up more buffers, and a higher memory bandwidth requirement
            from having more textures. The GPU will do the softening of the shadow, but since
            it is mostly idle anyway it will go unnoticed.
        </p>
        <img alt="the light distance buffer" src="@Url.Content("~/Content/images/raytracer/light-distance.png")" />
        <span class="image-caption">The light-distance buffer for a light as seen by the camera.</span>
        <img alt="the shadow buffer" src="@Url.Content("~/Content/images/raytracer/shadow-buffer.png")" />
        <span class="image-caption">The shadow buffer for a light as seen by the camera.</span>
        <h3>
            Unification</h3>
        <p>
            The current implementation allows multiple kinds of "scene objects", but it comes
            down to planes and meshes. As such, this involves some virtual method calls. The
            same holds true for lights; there is support for point lights (harsh shadows) and
            area lights (soft shadows). The plan is to unify this to avoid virtual method calls
            and to simplify the rendering pipeline.
        </p>
        <p>
            Initially, this meant that everything would be made up out of triangles. However
            I want to investigate building a renderer that uses voxels instead of triangles.
        </p>
    </section>
    <section title="gallery">
        <img src="@Url.Content("~/Content/images/raytracer/gallery-1.png")" />
        <img src="@Url.Content("~/Content/images/raytracer/gallery-2.png")" />
        <img src="@Url.Content("~/Content/images/raytracer/gallery-4.png")" />
    </section>
</div>
<script type="text/javascript">
    $(function () {
        $('#raytracer-sections').createTabs();
    });
</script>
